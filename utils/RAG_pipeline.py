# -*- coding: utf-8 -*-
"""Ollama_RAG_Latest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_5u7loUPJRLBxoFjxWwDmQUHBBBcpxwW
"""

!apt-get install lshw

!curl https://ollama.ai/install.sh | sh

import subprocess
process = subprocess.Popen("ollama serve", shell=True)

!ollama pull llama3.1:8b-instruct-q4_0
process2 = subprocess.Popen("ollama run llama3.1:8b-instruct-q4_0", shell=True)

!pip install langchain_community langgraph
# Add this after the existing pip install cell (cell #5)
!pip install langchain-ollama

from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

llm = ChatOllama(model="llama3.1:8b-instruct-q4_0", temperature=0)
prompt = PromptTemplate(
    template="""You are a sentiment classifier\n
    Here is the text: \n\n {text} \n\n
    Answer whether it conveys positive or negative or neutral sentiment.\n""",
    input_variables=["text"],
)

main_chain = prompt | llm

main_chain.invoke({"text":"hi"}).content

!pip install chromadb
!pip install langchain-huggingface
!pip install --upgrade sentence-transformers
!pip install --upgrade huggingface_hub

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# Initialize embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/e5-small-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# Simple text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""],
    length_function=len
)

def process_document_optimized(text, document_id, user_id):
    """Simple document processing without CSV assumptions"""

    print(f"DEBUG: Processing document length: {len(text)}")

    # Split text normally
    chunks = text_splitter.split_text(text)
    print(f"DEBUG: Created {len(chunks)} chunks")

    # Create simple metadata
    metadatas = []
    for i, chunk in enumerate(chunks):
        metadata = {
            "document_id": str(document_id),
            "user_id": str(user_id),
            "chunk_id": i,
            "chunk_length": len(chunk)
        }
        metadatas.append(metadata)

    # Create vectorstore
    vectorstore = Chroma.from_texts(
        texts=chunks,
        embedding=embeddings,
        metadatas=metadatas,
        collection_name=f"user_{user_id}_doc_{document_id}",
        persist_directory="./chroma_db"
    )

    return vectorstore, len(chunks)

# Add this as a new cell after Cell 9

def csv_aware_retrieve_documents(query, vectorstore, k=6):
    """CSV-optimized document retrieval"""

    # Keywords that suggest CSV/table queries
    csv_keywords = ["customer", "record", "data", "table", "database", "information",
                   "details", "list", "show", "find", "search", "who", "what",
                   "how many", "count", "total", "all"]

    query_lower = query.lower()
    is_likely_csv_query = any(keyword in query_lower for keyword in csv_keywords)

    print(f"DEBUG: Query: '{query}', CSV query: {is_likely_csv_query}")

    if is_likely_csv_query:
        try:
            # First try to get CSV content specifically
            csv_docs = vectorstore.similarity_search(
                f"customer data table {query}",  # Enhanced query
                k=k,
                filter={"is_csv": True}
            )
            print(f"DEBUG: Found {len(csv_docs)} CSV documents")

            if csv_docs:
                return csv_docs
            else:
                print("DEBUG: No CSV docs found with filter, trying without filter")

        except Exception as e:
            print(f"DEBUG: CSV-specific search failed: {e}")

    # Fallback to regular search with enhanced query
    enhanced_query = f"customer information data {query}"
    docs = vectorstore.similarity_search(enhanced_query, k=k)
    print(f"DEBUG: Fallback search returned {len(docs)} documents")

    return docs

def create_generic_rag_chain(vectorstore):
    """Create generic RAG chain without CSV bias"""

    def rag_function(query):
        print(f"DEBUG: Processing query: '{query}'")

        # Simple document retrieval without CSV assumptions
        docs = vectorstore.similarity_search(query, k=6)

        if not docs:
            return "I couldn't find any relevant information in the document."

        print(f"DEBUG: Retrieved {len(docs)} documents")

        # Clean context without CSV assumptions
        context = ""
        for i, doc in enumerate(docs):
            context += f"--- SECTION {i+1} ---\n"
            context += doc.page_content + "\n\n"

        # Generic prompt template
        template = f"""Based on the following document content, answer the question accurately:

{context}

Question: {query}

Answer based only on the provided content. If the information isn't available in the document, say "I don't have enough information to answer that question."""

        try:
            response = llm.invoke(template).content
            return response
        except Exception as e:
            print(f"DEBUG: LLM error: {e}")
            return f"I encountered an error generating the response: {str(e)}"

    return rag_function

# Cell 10 - Simplified retrieval
def optimized_retrieve_documents(query, vectorstore, k=6):
    """Simplified document retrieval"""

    # Check for structured data keywords
    structured_keywords = ["table", "data", "numbers", "list", "compare", "statistics"]
    wants_structured = any(keyword in query.lower() for keyword in structured_keywords)

    if wants_structured:
        try:
            # Get structured content first
            structured_docs = vectorstore.similarity_search(
                query,
                k=k//2,
                filter={"is_structured": True}
            )

            # Get regular content
            regular_docs = vectorstore.similarity_search(query, k=k//2)

            # Combine
            all_docs = structured_docs + [d for d in regular_docs if d not in structured_docs]
            return all_docs[:k]

        except:
            # Fallback to regular search
            pass

    # Regular MMR search
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": k*2, "lambda_mult": 0.7}
    )

    return retriever.get_relevant_documents(query)

def create_optimized_rag_chain(vectorstore):
    """Create optimized RAG chain"""

    def rag_function(query):
        # Detect query type
        query_lower = query.lower()
        is_summary = any(word in query_lower for word in ["summarize", "summary", "overview"])

        # Retrieve documents
        k = 8 if is_summary else 6
        docs = optimized_retrieve_documents(query, vectorstore, k)

        # Format context
        if is_summary:
            context = "DOCUMENT CONTENT:\n\n"
            for i, doc in enumerate(docs):
                context += f"Section {i+1}: {doc.page_content}\n\n"
        else:
            context = "\n\n".join([doc.page_content for doc in docs])

        # Create prompt
        if is_summary:
            template = f"""Provide a comprehensive summary based on these document sections:

{context}

Create a well-structured summary covering the main topics and key details.

User request: {query}"""
        else:
            template = f"""Answer based only on the provided context:

{context}

If the answer isn't in the context, say "I don't have enough information."

Question: {query}"""

        # Generate response
        response = llm.invoke(template).content
        return response

    return rag_function

# Ollama Health Monitor - Run this cell periodically
def monitor_ollama():
    try:
        # Check if process is running
        result = subprocess.run(['pgrep', '-f', 'ollama'], capture_output=True, text=True)
        if not result.stdout.strip():
            print("ðŸ”„ Ollama stopped. Restarting...")
            subprocess.Popen("ollama serve", shell=True)
            time.sleep(10)

        # Test LLM response
        test_response = llm.invoke("Hello")
        print(f"âœ… Ollama is healthy. Test response: {test_response.content[:30]}...")

    except Exception as e:
        print(f"âŒ Ollama health check failed: {e}")
        print("ðŸ”„ Attempting to restart Ollama...")
        subprocess.Popen("ollama serve", shell=True)

monitor_ollama()

# Cell 87 - Fixed Flask app with better large document handling

from datetime import datetime
from zoneinfo import ZoneInfo
from flask import Flask, request, jsonify
import time
import threading
import os
import traceback
import re

!pip install pyngrok
from pyngrok import ngrok
from langchain_ollama import ChatOllama

# Initialize the LLM
llm = ChatOllama(
    model="llama3.1:8b-instruct-q4_0",
    temperature=0
)

app = Flask(__name__)

# Storage
document_vectorstores = {}
conversation_history = {}

# Optimized retrieval function
def optimized_retrieve_documents(query, vectorstore, k=6):
    """Simplified document retrieval"""

    # Check for structured data keywords
    structured_keywords = ["table", "data", "numbers", "list", "compare", "statistics"]
    wants_structured = any(keyword in query.lower() for keyword in structured_keywords)

    if wants_structured:
        try:
            # Get structured content first
            structured_docs = vectorstore.similarity_search(
                query,
                k=k//2,
                filter={"is_structured": True}
            )

            # Get regular content
            regular_docs = vectorstore.similarity_search(query, k=k//2)

            # Combine
            all_docs = structured_docs + [d for d in regular_docs if d not in structured_docs]
            return all_docs[:k]

        except:
            # Fallback to regular search
            pass

    # Regular MMR search
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": k*2, "lambda_mult": 0.7}
    )

    return retriever.get_relevant_documents(query)

def create_optimized_rag_chain(vectorstore):
    """Create optimized RAG chain"""

    def rag_function(query):
        # Detect query type
        query_lower = query.lower()
        is_summary = any(word in query_lower for word in ["summarize", "summary", "overview"])

        # Retrieve documents
        k = 8 if is_summary else 6
        docs = optimized_retrieve_documents(query, vectorstore, k)

        # Format context
        if is_summary:
            context = "DOCUMENT CONTENT:\n\n"
            for i, doc in enumerate(docs):
                context += f"Section {i+1}: {doc.page_content}\n\n"
        else:
            context = "\n\n".join([doc.page_content for doc in docs])

        # Create prompt
        if is_summary:
            template = f"""Provide a comprehensive summary based on these document sections:

{context}

Create a well-structured summary covering the main topics and key details.

User request: {query}"""
        else:
            template = f"""Answer based only on the provided context:

{context}

If the answer isn't in the context, say "I don't have enough information."

Question: {query}"""

        # Generate response
        response = llm.invoke(template).content
        return response

    return rag_function

@app.route('/healthcheck', methods=['GET'])
def healthcheck():
    """Simple endpoint to check if the API is running"""
    return jsonify({"status": "ok", "model": "llama3.1:8b-instruct-q4_0"})

# Replace the process_document_api function in Cell 87

@app.route('/process_document', methods=['POST'])
def process_document_api():
    """Fixed document processing API with proper metadata handling"""
    try:
        data = request.json
        text = data.get('text', '')
        document_id = data.get('document_id', '')
        user_id = data.get('user_id', '')

        if not all([text, document_id, user_id]):
            return jsonify({"error": "Missing required fields"}), 400

        print(f"Processing document {document_id} - Length: {len(text)}")

        # Use optimized processing with fixed metadata
        vectorstore, chunk_count = process_document_optimized(text, document_id, user_id)

        # Force persistence for large documents
        try:
            vectorstore.persist()
            print(f"DEBUG: Persisted vectorstore for {document_id}")
        except Exception as persist_error:
            print(f"DEBUG: Persist warning: {persist_error}")

        # Store in memory
        key = f"{user_id}_{document_id}"
        document_vectorstores[key] = vectorstore

        # Test the vectorstore immediately
        try:
            test_docs = vectorstore.similarity_search("customer data", k=5)
            print(f"DEBUG: Vectorstore test successful - found {len(test_docs)} docs")

            # Calculate stats safely
            structured_count = sum(1 for doc in test_docs if doc.metadata.get("is_csv", False))

            return jsonify({
                "status": "success",
                "chunks": chunk_count,
                "structured_chunks": structured_count,
                "text_chunks": chunk_count - structured_count,
                "message": f"Processed {chunk_count} chunks successfully",
                "vectorstore_key": key
            })

        except Exception as test_error:
            print(f"DEBUG: Vectorstore test failed: {test_error}")
            return jsonify({
                "status": "success",
                "chunks": chunk_count,
                "message": f"Processed {chunk_count} chunks successfully (test failed)",
                "warning": str(test_error)
            })

    except Exception as e:
        print(f"Error: {str(e)}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/finalize_document', methods=['POST'])
def finalize_document():
    """Fixed finalize endpoint with better error handling"""
    data = request.json
    document_id = data.get('document_id', '')
    user_id = data.get('user_id', '')

    if not all([document_id, user_id]):
        return jsonify({"error": "Missing required fields"}), 400

    key = f"{user_id}_{document_id}"
    collection_name = f"user_{user_id}_doc_{document_id}"

    print(f"DEBUG: Finalizing document {key}")
    print(f"DEBUG: Collection name: {collection_name}")

    try:
        # First check if it's already in memory
        if key in document_vectorstores:
            try:
                docs = document_vectorstores[key].similarity_search("test", k=10)
                print(f"DEBUG: Found {len(docs)} docs in memory")
                return jsonify({
                    "status": "success",
                    "chunks": len(docs),
                    "message": "Document finalized successfully (from memory)"
                })
            except Exception as memory_error:
                print(f"DEBUG: Memory vectorstore failed: {memory_error}")
                # Continue to disk loading

        # Try to load from disk with multiple attempts
        vectorstore = None
        for attempt in range(3):
            try:
                print(f"DEBUG: Attempt {attempt + 1} to load from disk")

                # Try different ways to load the vectorstore
                if attempt == 0:
                    # Standard loading
                    vectorstore = Chroma(
                        collection_name=collection_name,
                        embedding_function=embeddings,
                        persist_directory="./chroma_db"
                    )
                elif attempt == 1:
                    # Try with explicit creation
                    vectorstore = Chroma(
                        collection_name=collection_name,
                        embedding_function=embeddings,
                        persist_directory="./chroma_db"
                    )
                    # Force load
                    vectorstore._collection.get()
                else:
                    # Last attempt - check if collection exists
                    import chromadb
                    client = chromadb.PersistentClient(path="./chroma_db")
                    collections = client.list_collections()
                    collection_names = [c.name for c in collections]
                    print(f"DEBUG: Available collections: {collection_names}")

                    if collection_name in collection_names:
                        vectorstore = Chroma(
                            collection_name=collection_name,
                            embedding_function=embeddings,
                            persist_directory="./chroma_db"
                        )
                    else:
                        # Try to find similar collection names
                        similar_names = [name for name in collection_names if user_id in name and document_id in name]
                        if similar_names:
                            print(f"DEBUG: Found similar collection: {similar_names[0]}")
                            vectorstore = Chroma(
                                collection_name=similar_names[0],
                                embedding_function=embeddings,
                                persist_directory="./chroma_db"
                            )
                        else:
                            raise Exception(f"No collections found for user {user_id} and document {document_id}")

                # Test the vectorstore
                docs = vectorstore.similarity_search("test", k=10)
                if docs:
                    print(f"DEBUG: Successfully loaded {len(docs)} docs on attempt {attempt + 1}")
                    document_vectorstores[key] = vectorstore
                    return jsonify({
                        "status": "success",
                        "chunks": len(docs),
                        "message": f"Document finalized successfully (from disk, attempt {attempt + 1})"
                    })
                else:
                    print(f"DEBUG: Vectorstore loaded but no docs found on attempt {attempt + 1}")

            except Exception as attempt_error:
                print(f"DEBUG: Attempt {attempt + 1} failed: {attempt_error}")
                if attempt == 2:  # Last attempt
                    raise attempt_error
                time.sleep(1)  # Wait before retry

        # If we get here, all attempts failed
        return jsonify({
            "status": "error",
            "error": "Document not found after multiple attempts",
            "message": f"Could not find or load document {document_id}",
            "suggestion": "Document may need to be reprocessed",
            "debug_info": {
                "key": key,
                "collection_name": collection_name,
                "in_memory": key in document_vectorstores
            }
        }), 404

    except Exception as e:
        print(f"DEBUG: Finalize error: {str(e)}")
        traceback.print_exc()
        return jsonify({
            "status": "error",
            "error": str(e),
            "message": "Failed to finalize document",
            "debug_info": {
                "key": key,
                "collection_name": collection_name
            }
        }), 500

# Replace the /generate route in Cell 87

@app.route('/generate', methods=['POST'])
def generate_response():
    data = request.json
    query = data.get('query', '')
    document_id = data.get('document_id', '')
    user_id = data.get('user_id', '')

    if not all([query, document_id, user_id]):
        return jsonify({"error": "Missing required fields"}), 400

    try:
        key = f"{user_id}_{document_id}"

        # Load vectorstore if needed
        if key not in document_vectorstores:
            try:
                vectorstore = Chroma(
                    collection_name=f"user_{user_id}_doc_{document_id}",
                    embedding_function=embeddings,
                    persist_directory="./chroma_db"
                )
                document_vectorstores[key] = vectorstore
            except Exception as e:
                return jsonify({"error": f"Document not found: {str(e)}"}), 404

        # Use generic RAG chain (not CSV-optimized)
        rag_chain = create_generic_rag_chain(document_vectorstores[key])
        response = rag_chain(query)

        # Update conversation history
        if key not in conversation_history:
            conversation_history[key] = []

        conversation_history[key].extend([
            {"role": "user", "content": query},
            {"role": "assistant", "content": response}
        ])

        # Keep last 10 messages
        if len(conversation_history[key]) > 10:
            conversation_history[key] = conversation_history[key][-10:]

        return jsonify({"response": response})

    except Exception as e:
        print(f"Generation error: {str(e)}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/document_status', methods=['GET'])
def document_status():
    """Check document status with better debugging"""
    document_id = request.args.get('document_id')
    user_id = request.args.get('user_id')

    if not all([document_id, user_id]):
        return jsonify({"error": "Missing document_id or user_id"}), 400

    key = f"{user_id}_{document_id}"
    collection_name = f"user_{user_id}_doc_{document_id}"

    # Check if document is available in memory
    if key in document_vectorstores:
        try:
            docs = document_vectorstores[key].similarity_search("test", k=10)
            return jsonify({
                "status": "ready",
                "in_memory": True,
                "chunks": len(docs),
                "collection_name": collection_name
            })
        except Exception as e:
            return jsonify({
                "status": "error",
                "in_memory": True,
                "error": str(e),
                "chunks": 0
            })

    # Try loading from disk
    try:
        vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory="./chroma_db"
        )
        docs = vectorstore.similarity_search("test", k=10)
        document_vectorstores[key] = vectorstore
        return jsonify({
            "status": "ready",
            "in_memory": True,
            "chunks": len(docs),
            "collection_name": collection_name
        })
    except Exception as e:
        # List available collections for debugging
        try:
            import chromadb
            client = chromadb.PersistentClient(path="./chroma_db")
            collections = [c.name for c in client.list_collections()]
            return jsonify({
                "status": "not_found",
                "error": str(e),
                "available_collections": collections,
                "looking_for": collection_name
            }), 404
        except:
            return jsonify({"status": "not_found", "error": str(e)}), 404

@app.route('/debug/collections', methods=['GET'])
def debug_collections():
    """Debug endpoint to see all collections"""
    try:
        import chromadb
        client = chromadb.PersistentClient(path="./chroma_db")
        collections = client.list_collections()

        collection_info = []
        for collection in collections:
            try:
                count = collection.count()
                collection_info.append({
                    "name": collection.name,
                    "count": count
                })
            except:
                collection_info.append({
                    "name": collection.name,
                    "count": "unknown"
                })

        return jsonify({
            "collections": collection_info,
            "total_collections": len(collections),
            "in_memory_vectorstores": list(document_vectorstores.keys())
        })
    except Exception as e:
        return jsonify({"error": str(e)})

# Start ngrok and Flask
ngrok.set_auth_token("YOUR_AUTH_TOKEN")
public_url = ngrok.connect(5000)
local_time = datetime.now(ZoneInfo("Asia/Kolkata"))
print(f"API accessible at: {public_url}\n{local_time.strftime('%Y-%m-%d %H:%M:%S')}")

# Run the Flask app
app.run(host='0.0.0.0', port=5000)

# Evaluation tools for document processing quality
def evaluate_chunk_quality(document_id, user_id):
    """Evaluate the quality of document chunking"""
    key = f"{user_id}_{document_id}"

    # Basic stats
    stats = {
        "total_chunks": 0,
        "avg_chunk_length": 0,
        "empty_chunks": 0,
        "short_chunks": 0,  # Less than 100 chars
        "content_distribution": {}
    }

    # Try to get vectorstore
    try:
        if key in document_vectorstores:
            vectorstore = document_vectorstores[key]
        else:
            vectorstore = Chroma(
                collection_name=f"user_{user_id}_doc_{document_id}",
                embedding_function=embeddings,
                persist_directory="./chroma_db"
            )

        # Get all chunks
        all_docs = vectorstore.similarity_search(
            "test document content",
            k=100  # Get up to 100 chunks for evaluation
        )

        if not all_docs:
            return {"error": "No chunks found", "stats": stats}

        # Calculate stats
        total_length = 0
        chunk_count = len(all_docs)
        empty_count = 0
        short_count = 0
        positions = {"beginning": 0, "middle": 0, "end": 0, "unknown": 0}

        for doc in all_docs:
            length = len(doc.page_content)
            total_length += length

            if length == 0:
                empty_count += 1
            elif length < 100:
                short_count += 1

            # Track position distribution if available
            position = doc.metadata.get("position", "unknown")
            positions[position] = positions.get(position, 0) + 1

        stats["total_chunks"] = chunk_count
        stats["avg_chunk_length"] = total_length / chunk_count if chunk_count > 0 else 0
        stats["empty_chunks"] = empty_count
        stats["short_chunks"] = short_count
        stats["content_distribution"] = positions

        return {
            "status": "success",
            "stats": stats,
            "sample_chunks": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                } for doc in all_docs[:3]
            ]
        }
    except Exception as e:
        return {"error": str(e), "stats": stats}

# Example usage:
evaluate_chunk_quality("2", "abcde")

# Test RAG performance on various query types
def test_rag_performance(document_id, user_id):
    """Test the RAG system on different query types to evaluate performance"""
    key = f"{user_id}_{document_id}"

    if key not in document_vectorstores:
        try:
            # Try to load from disk
            vectorstore = Chroma(
                collection_name=f"user_{user_id}_doc_{document_id}",
                embedding_function=embeddings,
                persist_directory="./chroma_db"
            )
            document_vectorstores[key] = vectorstore
        except Exception as e:
            return {"error": f"Failed to load document: {str(e)}"}

    # Define test queries of different types
    test_queries = {
        "factual": "What are the main topics covered in this document?",
        "summarization": "Summarize the key points of this document.",
        "specific": "What information does the document provide about [TOPIC]?",  # Replace [TOPIC] based on content
        "comparison": "Compare the different approaches mentioned in the document."
    }

    # Create the RAG chain
    rag_chain = create_rag_chain(document_vectorstores[key])

    # Run tests
    results = {}
    for query_type, query in test_queries.items():
        try:
            # Replace [TOPIC] with a potential topic from document metadata
            if "[TOPIC]" in query:
                # Try to extract potential topic from metadata
                docs = document_vectorstores[key].similarity_search("main topic", k=1)
                if docs:
                    potential_topic = docs[0].metadata.get("potential_header", "").strip()
                    if potential_topic:
                        query = query.replace("[TOPIC]", potential_topic)
                    else:
                        query = query.replace("[TOPIC]", "the main subject")

            # Time the response
            import time
            start_time = time.time()
            response = rag_chain(query)
            end_time = time.time()

            results[query_type] = {
                "query": query,
                "response": response,
                "response_time": round(end_time - start_time, 2)
            }
        except Exception as e:
            results[query_type] = {
                "query": query,
                "error": str(e)
            }

    return results

# Example usage:
# test_results = test_rag_performance("your_document_id", "your_user_id")
# for query_type, result in test_results.items():
#     print(f"===== {query_type.upper()} QUERY =====")
#     print(f"Query: {result['query']}")
#     if 'response' in result:
#         print(f"Response time: {result['response_time']}s")
#         print(f"Response:\n{result['response']}\n")
#     else:
#         print(f"Error: {result['error']}\n")